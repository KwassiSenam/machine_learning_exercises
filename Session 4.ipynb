{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d94d824",
   "metadata": {},
   "source": [
    "# Exo 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263ad5d3",
   "metadata": {},
   "source": [
    "#### Using a logistic Regression to predict the output(estimated salary) taking into account the age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606385a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pandas.plotting import scatter_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255c1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawClasses(X_set,y_set,c1,c2,title_name,x_label,y_label,classes,classifier=None):\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    plt.figure()\n",
    "\n",
    "    X1, X2 = np.meshgrid(np.arange(start = X_set[:, c1].min() - 1, stop = X_set[:, c1].max() + 1, step = 0.01),\n",
    "                         np.arange(start = X_set[:, c2].min() - 1, stop = X_set[:, c2].max() + 1, step = 0.01))\n",
    "    if classifier is not None:\n",
    "        plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\n",
    "                     alpha = 0.25, cmap = ListedColormap(classes))\n",
    "    plt.xlim(X1.min(), X1.max())\n",
    "    plt.ylim(X2.min(), X2.max())\n",
    "    for i, j in enumerate(np.unique(y_set)):\n",
    "        plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
    "                    c = ListedColormap(classes)(i), label = j)\n",
    "    plt.title('Classifier ({})'.format(title_name))\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921fb617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('./datasets/Social_Network_Ads.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af72e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, [2,3]].values\n",
    "y = dataset.iloc[:, 4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7adf1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d57f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier=LogisticRegression(random_state=0)\n",
    "classifier.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1539fa37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_preds = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18da6dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d18e2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classifier.classes_)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e090a83d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scatter_matrix(dataset);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe797a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=('red', 'green')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbc8512",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the Training set results\n",
    "drawClasses(X_train, y_train,0,1,'Training set','Age','Estimated Salary',classes,classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f92edac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the test set results\n",
    "drawClasses(X_test, y_test,0,1,'Test set','Age','Estimated Salary',classes,classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37de96bd",
   "metadata": {},
   "source": [
    "# Exo 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29cc0593",
   "metadata": {},
   "source": [
    "#### Using a logistic Regression to predict wheter or not a patient has a diabete."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d03353e",
   "metadata": {},
   "source": [
    "* pima-indians-diabetes dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c082eaa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './datasets/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df1 = pd.read_csv(filename, names=names, delimiter=\",\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45009e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop('class', axis=1)\n",
    "y = df1['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5507e008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0baf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression(random_state=0)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0a9f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9a1c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d43675",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33343ee0",
   "metadata": {},
   "source": [
    "* Iris Plant dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a5376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './datasets/iris_proc.data.csv'\n",
    "names = ['sepal length in cm', 'sepal width in cm', 'petal length in cm', 'petal width in cm', 'class']\n",
    "df2 = pd.read_csv(filename, names=names, delimiter=\",\")\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68af0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df2.drop('class', axis=1)\n",
    "y = df2['class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d21358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d358e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "# Fitting classifier to the Training set\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model=LogisticRegression(random_state=0)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805ff3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_preds = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67b4f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a922c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "cm = confusion_matrix(y_test, y_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=model.classes_)\n",
    "\n",
    "disp.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2825bb7",
   "metadata": {},
   "source": [
    "# Exo 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e4339b",
   "metadata": {},
   "source": [
    "#### This data set contains details of a bank's customers and the target variable is a binary variable(Exited) reflecting the fact whether the customer left the bank (closed his account) or he continues to be a customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556f557",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Importing the dataset\n",
    "dataset = pd.read_csv('./datasets/Churn_Modelling.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6c8ee",
   "metadata": {},
   "source": [
    "* Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d37578a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 3:13].values\n",
    "y = dataset.iloc[:, 13].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002b8387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical data\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n",
    "#label encoder uses number sequencing new number for every category\n",
    "labelencoder_X_1 = LabelEncoder()\n",
    "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1]) #geography\n",
    "labelencoder_X_2 = LabelEncoder()\n",
    "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2]) #Gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4130d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:, 2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c47837",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "onehotencoder = ColumnTransformer([(\"Geography\",OneHotEncoder(),[1])], remainder='passthrough')\n",
    "X = onehotencoder.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f8d261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67bc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3028722",
   "metadata": {},
   "source": [
    "* let's make the ANN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36265a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375adfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "classifier = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 12))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "classifier.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9665e87b",
   "metadata": {},
   "source": [
    "* Making predictions and evaluating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb075ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2582fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1a4a8b",
   "metadata": {},
   "source": [
    "## Exo 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde32adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test = [600, 0, 1, 40, 3, 60000, 2, 1, 1, 50000]\n",
    "df_input = pd.DataFrame(columns=['CreditScore', 'Geography',\n",
    "       'Gender', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard',\n",
    "       'IsActiveMember', 'EstimatedSalary'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcff331",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_input.loc[len(df_input.index)] = input_test\n",
    "df_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e664b052",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_final = sc.fit_transform(df_input.values)\n",
    "input_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2f6797",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(input_final)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b5275d",
   "metadata": {},
   "source": [
    "## Exo 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f52c02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = './datasets/pima-indians-diabetes.data.csv'\n",
    "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
    "df1 = pd.read_csv(filename, names=names, delimiter=\",\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc2ba02",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df1.drop('class', axis=1).values\n",
    "y = df1['class'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ed9df",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b824e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the dataset into the Training set and Test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd76f192",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf04758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Keras libraries and packages\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49533a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialising the ANN\n",
    "model = Sequential()\n",
    "\n",
    "# Adding the input layer and the first hidden layer\n",
    "model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 8))\n",
    "\n",
    "# Adding the second hidden layer\n",
    "model.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "\n",
    "# Adding the output layer\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Fitting the ANN to the Training set\n",
    "model.fit(X_train, y_train, batch_size = 10, epochs = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f81d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the Test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcefc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making the Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
